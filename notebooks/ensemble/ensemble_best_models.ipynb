{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00a811fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded files:\n",
      " - submission_member1 | rows: 1674896\n",
      " - submission2_dedup | rows: 1674896\n",
      " - submission3 | rows: 1674896\n",
      " - submission4 | rows: 1674896\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "FILES = [\n",
    "    \"submission_member1.csv\",\n",
    "    \"submission2_dedup.csv\",\n",
    "    \"submission3.csv\",\n",
    "    \"submission4.csv\",\n",
    "]\n",
    "\n",
    "ID_COL = \"id\"\n",
    "PRED_COL = \"binds\"\n",
    "\n",
    "dfs = []\n",
    "for path in FILES:\n",
    "    df = pd.read_csv(path)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    if ID_COL not in df.columns or PRED_COL not in df.columns:\n",
    "        raise ValueError(f\"{path}: must contain columns '{ID_COL}' and '{PRED_COL}'. Got: {df.columns.tolist()}\")\n",
    "\n",
    "    if not df[ID_COL].is_unique:\n",
    "        dup = df[df[ID_COL].duplicated()][ID_COL].head(5).tolist()\n",
    "        raise ValueError(f\"{path}: '{ID_COL}' has duplicates. Example duplicated ids: {dup}\")\n",
    "\n",
    "    mn, mx = df[PRED_COL].min(), df[PRED_COL].max()\n",
    "    if not (np.isfinite(mn) and np.isfinite(mx)):\n",
    "        raise ValueError(f\"{path}: non-finite predictions in '{PRED_COL}'\")\n",
    "    if mn < 0 or mx > 1:\n",
    "        print(f\"Warning: {path} '{PRED_COL}' out of [0,1] range: min={mn}, max={mx}\")\n",
    "\n",
    "    model_name = path.replace(\".csv\", \"\")\n",
    "    df = df[[ID_COL, PRED_COL]].rename(columns={PRED_COL: model_name})\n",
    "    dfs.append(df)\n",
    "\n",
    "print(\"Loaded files:\")\n",
    "for d in dfs:\n",
    "    print(\" -\", d.columns[1], \"| rows:\", len(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1baf67b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = dfs[0]\n",
    "for df in dfs[1:]:\n",
    "    merged = merged.merge(df, on=ID_COL, how=\"inner\")\n",
    "\n",
    "n_expected = len(dfs[0])\n",
    "if len(merged) != n_expected:\n",
    "    print(f\"\\n[!] Merge size differs: merged={len(merged)} vs first_file={n_expected}\")\n",
    "    base_ids = set(dfs[0][ID_COL])\n",
    "    for df in dfs[1:]:\n",
    "        other_ids = set(df[ID_COL])\n",
    "        print(f\"Missing in {df.columns[1]} relative to base: {len(base_ids - other_ids)}\")\n",
    "        print(f\"Extra in {df.columns[1]} relative to base: {len(other_ids - base_ids)}\")\n",
    "    raise ValueError(\"Submissions do not align by 'id' (different test files or corrupted).\")\n",
    "\n",
    "pred_cols = [c for c in merged.columns if c != ID_COL]\n",
    "\n",
    "merged[\"binds_mean\"] = merged[pred_cols].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "408d70aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_avg(df_preds: pd.DataFrame) -> np.ndarray:\n",
    "    ranks = np.zeros(len(df_preds), dtype=float)\n",
    "    for c in df_preds.columns:\n",
    "        ranks += df_preds[c].rank(method=\"average\").to_numpy()\n",
    "    ranks /= len(df_preds.columns)\n",
    "    ranks = (ranks - ranks.min()) / (ranks.max() - ranks.min() + 1e-12)\n",
    "    return ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "360bc324",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[\"binds_rankavg\"] = rank_avg(merged[pred_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac629282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model correlation (Pearson):\n",
      "                    submission_member1  submission2_dedup  submission3  \\\n",
      "submission_member1            1.000000           0.581731     0.581731   \n",
      "submission2_dedup             0.581731           1.000000     1.000000   \n",
      "submission3                   0.581731           1.000000     1.000000   \n",
      "submission4                   0.705325           0.479336     0.479336   \n",
      "\n",
      "                    submission4  \n",
      "submission_member1     0.705325  \n",
      "submission2_dedup      0.479336  \n",
      "submission3            0.479336  \n",
      "submission4            1.000000  \n"
     ]
    }
   ],
   "source": [
    "out_mean = merged[[ID_COL, \"binds_mean\"]].rename(columns={\"binds_mean\": PRED_COL})\n",
    "out_rank = merged[[ID_COL, \"binds_rankavg\"]].rename(columns={\"binds_rankavg\": PRED_COL})\n",
    "\n",
    "out_mean.to_csv(\"submission_ensemble_mean_4_models.csv\", index=False)\n",
    "out_rank.to_csv(\"submission_ensemble_rankavg_4_models.csv\", index=False)\n",
    "\n",
    "# print(\"\\nSaved:\")\n",
    "# print(\" - submission_ensemble_mean.csv\")\n",
    "# print(\" - submission_ensemble_rankavg.csv\")\n",
    "\n",
    "corr = merged[pred_cols].corr()\n",
    "print(\"\\nModel correlation (Pearson):\")\n",
    "print(corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3294cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded files:\n",
      " - submission_member1 | rows: 1674896\n",
      " - submission2_dedup | rows: 1674896\n",
      " - submission_gnn_kfold | rows: 1674896\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "FILES = [\n",
    "    \"submission_member1.csv\",\n",
    "    \"submission2_dedup.csv\",\n",
    "    # \"submission3.csv\",\n",
    "    # \"submission4.csv\",\n",
    "    \"submission_gnn_kfold.csv\"\n",
    "]\n",
    "\n",
    "ID_COL = \"id\"\n",
    "PRED_COL = \"binds\"\n",
    "\n",
    "dfs = []\n",
    "for path in FILES:\n",
    "    df = pd.read_csv(path)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    if ID_COL not in df.columns or PRED_COL not in df.columns:\n",
    "        raise ValueError(f\"{path}: must contain columns '{ID_COL}' and '{PRED_COL}'. Got: {df.columns.tolist()}\")\n",
    "\n",
    "    if not df[ID_COL].is_unique:\n",
    "        dup = df[df[ID_COL].duplicated()][ID_COL].head(5).tolist()\n",
    "        raise ValueError(f\"{path}: '{ID_COL}' has duplicates. Example duplicated ids: {dup}\")\n",
    "\n",
    "    mn, mx = df[PRED_COL].min(), df[PRED_COL].max()\n",
    "    if not (np.isfinite(mn) and np.isfinite(mx)):\n",
    "        raise ValueError(f\"{path}: non-finite predictions in '{PRED_COL}'\")\n",
    "    if mn < 0 or mx > 1:\n",
    "        print(f\"Warning: {path} '{PRED_COL}' out of [0,1] range: min={mn}, max={mx}\")\n",
    "\n",
    "    model_name = path.replace(\".csv\", \"\")\n",
    "    df = df[[ID_COL, PRED_COL]].rename(columns={PRED_COL: model_name})\n",
    "    dfs.append(df)\n",
    "\n",
    "print(\"Loaded files:\")\n",
    "for d in dfs:\n",
    "    print(\" -\", d.columns[1], \"| rows:\", len(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e495db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = dfs[0]\n",
    "for df in dfs[1:]:\n",
    "    merged = merged.merge(df, on=ID_COL, how=\"inner\")\n",
    "\n",
    "n_expected = len(dfs[0])\n",
    "if len(merged) != n_expected:\n",
    "    print(f\"\\n[!] Merge size differs: merged={len(merged)} vs first_file={n_expected}\")\n",
    "    base_ids = set(dfs[0][ID_COL])\n",
    "    for df in dfs[1:]:\n",
    "        other_ids = set(df[ID_COL])\n",
    "        print(f\"Missing in {df.columns[1]} relative to base: {len(base_ids - other_ids)}\")\n",
    "        print(f\"Extra in {df.columns[1]} relative to base: {len(other_ids - base_ids)}\")\n",
    "    raise ValueError(\"Submissions do not align by 'id' (different test files or corrupted).\")\n",
    "\n",
    "pred_cols = [c for c in merged.columns if c != ID_COL]\n",
    "\n",
    "merged[\"binds_mean\"] = merged[pred_cols].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7890b0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_avg(df_preds: pd.DataFrame) -> np.ndarray:\n",
    "    ranks = np.zeros(len(df_preds), dtype=float)\n",
    "    for c in df_preds.columns:\n",
    "        ranks += df_preds[c].rank(method=\"average\").to_numpy()\n",
    "    ranks /= len(df_preds.columns)\n",
    "    ranks = (ranks - ranks.min()) / (ranks.max() - ranks.min() + 1e-12)\n",
    "    return ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3fd0b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[\"binds_rankavg\"] = rank_avg(merged[pred_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ba901ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model correlation (Pearson):\n",
      "                      submission_member1  submission2_dedup  \\\n",
      "submission_member1              1.000000           0.581731   \n",
      "submission2_dedup               0.581731           1.000000   \n",
      "submission_gnn_kfold            0.705237           0.450031   \n",
      "\n",
      "                      submission_gnn_kfold  \n",
      "submission_member1                0.705237  \n",
      "submission2_dedup                 0.450031  \n",
      "submission_gnn_kfold              1.000000  \n"
     ]
    }
   ],
   "source": [
    "out_mean = merged[[ID_COL, \"binds_mean\"]].rename(columns={\"binds_mean\": PRED_COL})\n",
    "out_rank = merged[[ID_COL, \"binds_rankavg\"]].rename(columns={\"binds_rankavg\": PRED_COL})\n",
    "\n",
    "out_mean.to_csv(\"submission_ensemble_mean_4_models.csv\", index=False)\n",
    "out_rank.to_csv(\"submission_ensemble_rankavg_4_models.csv\", index=False)\n",
    "\n",
    "# print(\"\\nSaved:\")\n",
    "# print(\" - submission_ensemble_mean.csv\")\n",
    "# print(\" - submission_ensemble_rankavg.csv\")\n",
    "\n",
    "corr = merged[pred_cols].corr()\n",
    "print(\"\\nModel correlation (Pearson):\")\n",
    "print(corr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
